{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entire file to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Metamorphosis.txt', 'r') as file:\n",
    "    file = file.read()\n",
    "lower = ''\n",
    "for i in file:\n",
    "    lower += i.lower()\n",
    "with open('lower.txt', 'w') as low:\n",
    "    low.write(lower)\n",
    "import os\n",
    "os.system('subl lower.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify english typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "with open('cf.txt', 'r') as file:\n",
    "    file = file.readlines()\n",
    "a = [i.replace('\\n' ,'') for i in file]\n",
    "\n",
    "# count of all mistakes (with replacement)\n",
    "len(a)\n",
    "\n",
    "# count of unique mistakes (with replacement) \n",
    "len(set(a))\n",
    "\n",
    "# unique sorted mistakes\n",
    "sorted_unique = sorted(list(set(a)))\n",
    "\n",
    "# open your file\n",
    "with open('lower.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "sentences = sent_tokenize(text)\n",
    "# creat new file\n",
    "with open('identify.txt', 'w') as mistakes:\n",
    "    mistakes.write('  |-------------------|\\n')\n",
    "    mistakes.write('  |Total mistakes: ' + str(len(a)) + '|\\n')\n",
    "    mistakes.write('  |Unique mistakes: ' + str(len(set(a))) + '|\\n')\n",
    "    mistakes.write('  |-------------------|\\n')\n",
    "    mistakes.write('unique mistakes: \\n')\n",
    "    mistakes.write('\\n'.join([i for i in sorted_unique]))\n",
    "    mistakes.write('\\n')\n",
    "    for i in sorted_unique:\n",
    "        count = 0\n",
    "        mistakes.write('\\n\\n  ' + '*'*10 + i + '*'*10 + '  \\n')\n",
    "        for b in sentences:\n",
    "            splited = b.split()\n",
    "            if i in splited:\n",
    "                count += 1\n",
    "                index = splited.index(i)\n",
    "                to_check = splited[index-4:index+4]\n",
    "                if index < 4:\n",
    "                    to_check = splited[:index+4]\n",
    "                mistakes.write(str(count) + '---' + ' '.join(to_check).replace(i, '|||' + i + '|||') + '\\n')\n",
    "import os\n",
    "os.system('subl identify.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Sentences:  915\n",
      "new file name:  lower_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "def text_clean(filename, to_file = True):\n",
    "    import string\n",
    "    import re\n",
    "    from nltk import sent_tokenize\n",
    "    from nltk.stem import SnowballStemmer as snow\n",
    "    from nltk.corpus import stopwords\n",
    "    import os\n",
    "    \n",
    "    snow = snow('english')\n",
    "    with open(filename + '.txt', 'r') as file:\n",
    "        text = file.read()\n",
    "    sentences = sent_tokenize(text)\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    # remove stopWords, punctuations, stemmed\n",
    "    final = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        sentence = re.sub(cleanr, ' ', sentence)                 #Removing HTML tags\n",
    "        sentence = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "        sentence = re.sub(r'[.|,|)|(|\\|/]',r' ',sentence)        #Removing Punctuations\n",
    "        sentence = sentence.strip(' ')\n",
    "#         sentence = sentence.lower()\n",
    "        stemmed = [snow.stem(word) for word in sentence.split() if word not in stopwords.words('english')]\n",
    "        final.append(' '.join([word.translate(table) for word in stemmed]))\n",
    "    count = 0\n",
    "    for i,b in zip(sentences, final):\n",
    "        if i != b:\n",
    "            count += 1\n",
    "    print('Corrected Sentences: ', count)\n",
    "    if to_file:\n",
    "        to_txt = ''\n",
    "        for i in final:\n",
    "            to_txt += i + '\\n'\n",
    "        new_file_name = filename + '_cleaned.txt'\n",
    "        with open(new_file_name, 'w') as file:\n",
    "            file.write(to_txt)\n",
    "        print('new file name: ', new_file_name)\n",
    "        os.system('subl {}'.format(new_file_name))\n",
    "text_clean('lower', 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

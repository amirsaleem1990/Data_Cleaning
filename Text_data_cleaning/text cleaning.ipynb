{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://machinelearningmastery.com/clean-text-machine-learning-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Metamorphosis.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "# split into words by white space\n",
    "words = text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ab hame is text ko words me split karna h, or hayan hamary paas 3 options hen:\n",
    "### 1- Split by Whitespace (masla: We can see that punctuation is preserved (e.g. “wasn’t” and “armour-like“),\n",
    "#### >>> words = text.split()\n",
    "\n",
    "### 2- Split using regex (masla: What’s” is two words “What” and “s”)\n",
    "#### >>> import re\n",
    "#### >>> words = re.split(r'\\W+', text)\n",
    "\n",
    "### 3- Split by Whitespace and Remove Punctuation (“What’s” have become “Whats” but “armour-like” has become “armourlike“.)\n",
    "#### >>> import string\n",
    "#### >>> table = str.maketrans('', '', string.punctuation)\n",
    "#### >>> stripped = [w.translate(table) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25186\n",
      "3352\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in words]\n",
    "print(len(stripped))\n",
    "# 25186\n",
    "# same as print(len(words))\n",
    "\n",
    "print(len(set(stripped)))\n",
    "# 3352\n",
    "# fever from print(len(set(words))) <which is 4595>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: This means that the vocabulary will shrink in size, but some distinctions are lost (e.g. “Apple” the company vs “apple” the fruit is a commonly used example or ‘Bush’ is different than ‘bush’, while ‘Another’ has usually the same sense as ‘another’).\n",
    "### 1 or cheez bhi consider karna hoti h, wo ye k peragraph k beech me jo word capital ho us ko lower nahi karna q k bohot zyada chance h k wo ksi makhsoos wajah sy capital kya h, eg: US vs us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = [i.lower() for i in stripped]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Cleaning with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A good useful first step is to split the text into sentences.\n",
    "\n",
    "Some modeling tasks prefer input to be in the form of paragraphs or sentences, such as word2vec. You could first split your text into sentences, split each sentence into words, then save each sentence to file, one per line.\n",
    "\n",
    "NLTK provides the sent_tokenize() function to split text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of Metamorphosis, by Franz Kafka\n",
      "Translated by David Wyllie.\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the example, we can see that although the document is split into sentences, that each sentence still preserves the new line from the artificial wrap of the lines in the original document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Metamorphosis', ',', 'by', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', '.', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.', 'You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'www.gutenberg.net', '**', 'This', 'is', 'a', 'COPYRIGHTED', 'Project', 'Gutenberg', 'eBook', ',', 'Details', 'Below', '**', '**', 'Please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', '.', '**', 'Title', ':', 'Metamorphosis', 'Author', ':', 'Franz', 'Kafka', 'Translator', ':', 'David', 'Wyllie', 'Release', 'Date', ':', 'August', '16']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It splits tokens based on white space and punctuation. For example, commas and periods are taken as separate tokens. Contractions are split apart (e.g. “What’s” becomes “What” “‘s“). Quotes are kept, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Out Punctuation\n",
    "### We can filter out all tokens that we are not interested in, such as all standalone punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'metamorphosis', 'by', 'franz', 'kafka', 'translated', 'by', 'david', 'wyllie', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'this', 'is', 'a', 'copyrighted', 'project', 'gutenberg', 'ebook', 'details', 'below', 'please', 'follow', 'the', 'copyright', 'guidelines', 'in', 'this', 'file', 'title', 'metamorphosis', 'author', 'franz', 'kafka', 'translator', 'david', 'wyllie', 'release', 'date', 'august', 'ebook', 'first', 'posted', 'may', 'last', 'updated', 'may', 'language', 'english', 'start', 'of', 'this', 'project', 'gutenberg', 'ebook', 'metamorphosis', 'copyright']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "# remove all tokens that are not alphabetic\n",
    "words = [word.lower() for word in tokens if word.lower().isalpha()]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out Stop Words (and Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no', 'be', 's', 'was', 'some', \"doesn't\", 'doing', \"needn't\", 'been', 'she', 'they', 'for', 'yours', 're', 'it', 'on', 'below', 'under', \"don't\", 'yourselves', 'above', 'own', \"shan't\", 'haven', 'won', 'her', 'these', \"haven't\", \"shouldn't\", 'my', 'didn', 'there', 'an', 'y', 'as', 'those', 'again', 'if', 'ourselves', 'of', 'most', 'between', 'them', 'are', 'other', 'until', 'than', 'had', 'wouldn', \"weren't\", 'having', 'its', \"that'll\", 'how', 'i', 'you', \"you'll\", 'll', 'against', 'hers', 'by', 'up', 'herself', 'all', 'while', 'same', \"isn't\", 'that', 'is', 'both', 'to', 'we', \"hadn't\", \"aren't\", 'isn', 'should', 'their', 'our', 'hadn', 'couldn', 'himself', 'once', 'so', 'him', \"it's\", 'now', 'whom', 'being', 'just', 'did', 'will', 't', 'itself', 'theirs', 'when', 'has', 'wasn', \"wasn't\", \"mustn't\", 'over', 'with', 'not', 'out', 'more', 'aren', 'before', 'doesn', 'shouldn', 'ma', 'his', 'mightn', 'nor', 'hasn', \"mightn't\", 'after', 'do', 'myself', 'your', 'only', \"you'd\", 'needn', \"won't\", 'this', \"you're\", 'the', 'ain', 'because', 'themselves', 'ours', 'where', \"should've\", 'into', 'from', 'in', 'can', \"hasn't\", 'further', 'few', 'such', 'very', 'but', 'then', 'down', 'during', \"you've\", \"couldn't\", 'which', 'were', 'have', 'weren', 'and', 'any', \"she's\", 'here', 'through', 'each', 'm', 'yourself', 'o', \"didn't\", 'off', 'shan', 'what', 'me', 'or', 'a', 'does', 'about', 'mustn', 'd', 'am', 'who', \"wouldn't\", 'he', 'at', 'too', 've', 'why', 'don'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'metamorphosis', 'franz', 'kafka', 'translated', 'david', 'wyllie', 'ebook', 'use', 'anyone', 'anywhere', 'cost', 'almost', 'restrictions', 'whatsoever', 'may', 'copy', 'give', 'away', 'terms', 'project', 'gutenberg', 'license', 'included', 'ebook', 'online', 'copyrighted', 'project', 'gutenberg', 'ebook', 'details', 'please', 'follow', 'copyright', 'guidelines', 'file', 'title', 'metamorphosis', 'author', 'franz', 'kafka', 'translator', 'david', 'wyllie', 'release', 'date', 'august', 'ebook', 'first', 'posted', 'may', 'last', 'updated', 'may', 'language', 'english', 'start', 'project', 'gutenberg', 'ebook', 'metamorphosis', 'copyright', 'c', 'david', 'wyllie', 'metamorphosis', 'franz', 'kafka', 'translated', 'david', 'wyllie', 'one', 'morning', 'gregor', 'samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections']\n"
     ]
    }
   ],
   "source": [
    "words = [w for w in words if not w in stop_words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem Words\n",
    "\n",
    "### Stemming refers to the process of reducing each word to its root or base.\n",
    "### For example “fishing,” “fished,” “fisher” all reduce to the stem “fish.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'metamorphosi', 'franz', 'kafka', 'translat', 'david', 'wylli', 'ebook', 'use', 'anyon', 'anywher', 'cost', 'almost', 'restrict', 'whatsoev', 'may', 'copi', 'give', 'away', 'term', 'project', 'gutenberg', 'licens', 'includ', 'ebook', 'onlin', 'copyright', 'project', 'gutenberg', 'ebook', 'detail', 'pleas', 'follow', 'copyright', 'guidelin', 'file', 'titl', 'metamorphosi', 'author', 'franz', 'kafka', 'translat', 'david', 'wylli', 'releas', 'date', 'august', 'ebook', 'first', 'post', 'may', 'last', 'updat', 'may', 'languag', 'english', 'start', 'project', 'gutenberg', 'ebook', 'metamorphosi', 'copyright', 'c', 'david', 'wylli', 'metamorphosi', 'franz', 'kafka', 'translat', 'david', 'wylli', 'one', 'morn', 'gregor', 'samsa', 'woke', 'troubl', 'dream', 'found', 'transform', 'bed', 'horribl', 'vermin', 'lay', 'back', 'lift', 'head', 'littl', 'could', 'see', 'brown', 'belli', 'slightli', 'dome', 'divid', 'arch', 'stiff', 'section']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in words]\n",
    "print(stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips for Cleaning Text for Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, the field of natural language processing has been moving away from bag-of-word models and word encoding toward word embeddings.\n",
    "\n",
    "The benefit of word embeddings is that they encode each word into a dense vector that captures something about its relative meaning within the training text.\n",
    "\n",
    "This means that variations of words like case, spelling, punctuation, and so on will automatically be learned to be similar in the embedding space. In turn, this can mean that the amount of cleaning required from your text may be less and perhaps quite different to classical text cleaning.\n",
    "\n",
    "For example, it may no-longer make sense to stem words or remove punctuation for contractions.\n",
    "\n",
    "Tomas Mikolov is one of the developers of word2vec, a popular word embedding method. He suggests only very minimal text cleaning is required when learning a word embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
